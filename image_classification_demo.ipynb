{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as ski\n",
    "import skimage.io as skio\n",
    "import skimage.transform as sktransform\n",
    "from skimage.filters import threshold_otsu, threshold_sauvola\n",
    "from skimage.color import rgb2gray\n",
    "from IPython.core.display import Image, display\n",
    "from skimage import feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **scikit-learn**是一个Python第三方提供的非常强力的机器学习库，它包含了从数据预处理到训练模型的各个方面。在实战使用scikit-learn中可以极大的节省我们编写代码的时间以及减少我们的代码量，使我们有更多的精力去分析数据分布，调整模型和修改超参数。sklearn拥有可以用于监督和无监督学习的方法，一般来说监督学习使用的更多。[关于scikit-learn的介绍](https://zhuanlan.zhihu.com/p/33420189 \"知乎\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **导入预处理模块中的scale模块  将用于训练数据的标准化和归一化处理**    \n",
    ">  在机器学习领域中，不同评价指标（即特征向量中的不同特征就是所述的不同评价指标）往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。其中，最典型的就是数据的归一化处理。[引用博客链接](https://blog.csdn.net/zenghaitao0128/article/details/78361038)，具体关于标准化和归一化请参考[特征工程中的「归一化」有什么作用？ - 知乎](https://www.zhihu.com/question/20455227)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 批量提取GLCM, HOG,LBP特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_features_list(path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path : string :要进行特征提取的图片路径\n",
    "    Return:\n",
    "        feature_list : dict : 包含三种特征的字典\n",
    "    \"\"\"\n",
    "\n",
    "    origin_image = skio.imread(path)\n",
    "    origin_image = sktransform.rescale(origin_image, 0.01)\n",
    "    gray_image = rgb2gray(origin_image)\n",
    "    otsu_threshold = threshold_otsu(gray_image)\n",
    "    gray_image = ski.img_as_ubyte(gray_image)\n",
    "    bin_image = gray_image > otsu_threshold\n",
    "    # 此处分别使用了水平、竖直、以及45 和135 度方向求 GLCM特征\n",
    "    feature_glcm = feature.greycomatrix(gray_image, [3], [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4], levels=256)\n",
    "#     hog_feature_vector, hog_image = feature.hog(gray_image, orientations=8, pixels_per_cell=(5, 5),\n",
    "#                                                 cells_per_block=(1, 1), visualize=True, block_norm='L2-Hys',\n",
    "#                                                 feature_vector=True)\n",
    "    # 设置LBP 特征提取算法的参数\n",
    "#     radius = 3\n",
    "#     n_points = 8 * radius\n",
    "    #print(gray_image.dtype)\n",
    "#     feature_lbp = feature.local_binary_pattern(gray_image, n_points, radius, 'uniform')\n",
    "    feature_glcm_flattened = feature_glcm.flatten()\n",
    "#     feature_hog_flattened = hog_feature_vector\n",
    "#     feature_lbp_flattended = feature_lbp.flatten()\n",
    "    result = {'glcm': feature_glcm_flattened}\n",
    "              #, 'hog': feature_hog_flattened, 'lbp': feature_lbp_flattended}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 找出指定文件夹下的所有jpg图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def list_files(root_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root_path : string : 图片所在文件夹的路径 \n",
    "    Return:\n",
    "        file_list : list : 文件路径列表\n",
    "    \n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for file in os.listdir(root_path):\n",
    "        if file.endswith(\".png\"):\n",
    "            file_list.append(file)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 数据预处理\n",
    "> 数据降维\n",
    "> 数据标准化，归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "def apply_pca(features_list_dict, components_num=2):\n",
    "    result = {}\n",
    "    pca1 = PCA(n_components=components_num, copy=True, whiten=False)\n",
    "#     pca2 = PCA(n_components=components_num, copy=True, whiten=False)\n",
    "#     pca3 = PCA(n_components=components_num, copy=True, whiten=False)\n",
    "    glcm = np.array(features_list_dict['glcm']).astype(float)\n",
    "#     hog = np.array(features_list_dict['hog']).astype(float)\n",
    "#     lbp = np.array(features_list_dict['lbp']).astype(float)\n",
    "    glcm_pca = pca1.fit(glcm[:, 0:-1])\n",
    "#     hog_pca = pca2.fit(hog[:, 0:-1])\n",
    "#     lbp_pca = pca3.fit(lbp[:, 0:-1])\n",
    "    glcm_coef = []\n",
    "#     hog_coef = []\n",
    "#     lbp_coef = []\n",
    "    for i in range(components_num):\n",
    "        glcm_coef.append(tuple(glcm_pca.components_[i]))\n",
    "#         hog_coef.append(tuple(hog_pca.components_[i]))\n",
    "#         lbp_coef.append(tuple(lbp_pca.components_[i]))\n",
    "    glcm_coef_matrix = np.array(glcm_coef)\n",
    "#     hog_coef_matrix = np.array(hog_coef)\n",
    "#     lbp_coef_matrix = np.array(lbp_coef)\n",
    "    '''\n",
    "    np.save('glcm_coef_matrix.npy',glcm_coef_matrix)\n",
    "    np.save('hog_coef_matrix.npy',hog_coef_matrix)\n",
    "    np.save('lbp_coef_matrix.npy',lbp_coef_matrix)\n",
    "    '''\n",
    "    glcm_features_matrix = np.dot(glcm[:, 0:-1], glcm_coef_matrix.T)\n",
    "#     hog_features_matrix = np.dot(hog[:, 0:-1], hog_coef_matrix.T)\n",
    "#     lbp_features_matrix = np.dot(lbp[:, 0:-1], lbp_coef_matrix.T)\n",
    "    glcm_features_matrix_ = np.column_stack([scale(glcm_features_matrix), glcm[:, -1]])\n",
    "#     hog_features_matrix_ = np.column_stack([scale(hog_features_matrix), hog[:, -1]])\n",
    "#     lbp_features_matrix_ = np.column_stack([scale(lbp_features_matrix), lbp[:, -1]])\n",
    "    result['glcm'] = glcm_features_matrix_\n",
    "#     result['hog'] = hog_features_matrix_\n",
    "#     result['lbp'] = lbp_features_matrix_\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 构造训练集和测试集\n",
    "> 将带标签的各特征矩阵 打乱顺序之后进行划分   \n",
    "> 使用sklearn自带的数据集划分函数进行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_train_test_dataset(features_list_dict):\n",
    "    glcm_train_data = features_list_dict['glcm'][:,0:-1]\n",
    "#     hog_train_data = features_list_dict['hog'][:,0:-1]\n",
    "#     lbp_train_data = features_list_dict['lbp'][:,0:-1]\n",
    "    \n",
    "    label_data = features_list_dict['glcm'][:,-1]\n",
    "    train_data = np.column_stack([glcm_train_data])\n",
    "                                  #, hog_train_data, lbp_train_data])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data, label_data, test_size=0.3, random_state=0)\n",
    "    return x_train, x_test ,y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 选择分类器进行分类\n",
    "> - SVM\n",
    "> - LogisticRegression\n",
    "> - RandomForestClassifier\n",
    "> - AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    clfs = {'svm': svm.SVC(gamma='scale'),\n",
    "            'random_forest': RandomForestClassifier(n_estimators=50),\n",
    "            'adaboost': AdaBoostClassifier(n_estimators=50),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def classify(x_train, y_train, x_test, y_test):\n",
    "    for clf_key in clfs.keys():\n",
    "        clf = clfs[clf_key]\n",
    "        clf.fit(x_train, y_train.ravel())\n",
    "        score = clf.score(x_test, y_test.ravel())\n",
    "        print('the classifier is\\t :', clf_key, '\\t the score is :', score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 使用交叉验证的方式来训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "def by_cross_validate(x_train, x_test, y_train, y_test):\n",
    "    features_data = np.row_stack([x_train, x_test])\n",
    "    label_data = np.row_stack([np.array([y_train]).T, np.array([y_test]).T])\n",
    "    for clf_key in clfs.keys():\n",
    "        clf = clfs[clf_key]\n",
    "        scoring = ['accuracy']\n",
    "        scores = cross_validate(clf, features_data, label_data.ravel(), cv=6, scoring=scoring)\n",
    "        print('classifier:\\t', clf_key)\n",
    "        print('by_cross_validate test_accuracy score  :', scores['test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '11', '12', '13', '14', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [03:38<00:00, 14.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hog'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8d7bddb15e96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mpca_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_list_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_train_test_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mby_cross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-62219d3fbfa1>\u001b[0m in \u001b[0;36msplit_train_test_dataset\u001b[1;34m(features_list_dict)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_train_test_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_list_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mglcm_train_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_list_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'glcm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mhog_train_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_list_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hog'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mlbp_train_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_list_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lbp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'hog'"
     ]
    }
   ],
   "source": [
    "path = r'D:\\project\\image_classification_demo\\swedish-dataset-square-256'\n",
    "features_list_dict = {}\n",
    "features_list_dict['glcm'] = []\n",
    "features_list_dict['hog'] = []\n",
    "features_list_dict['lbp'] = []\n",
    "dirs = os.listdir(path)\n",
    "print(dirs)\n",
    "for d in tqdm(dirs):\n",
    "    file_list = list_files(os.path.join(path, d))\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(path, d, file)\n",
    "        result = get_features_list(file_path)\n",
    "        features_list_dict['glcm'].append(np.hstack([result['glcm'], d]))\n",
    "#         features_list_dict['hog'].append(np.hstack([result['hog'], d]))\n",
    "#         features_list_dict['lbp'].append(np.hstack([result['lbp'], d]))\n",
    "print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result = apply_pca(features_list_dict, components_num=10)\n",
    "x_train, x_test, y_train, y_test = split_train_test_dataset(pca_result)\n",
    "by_cross_validate(x_train, x_test, y_train, y_test)\n",
    "classify(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
